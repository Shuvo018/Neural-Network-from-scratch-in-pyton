# Why we need bias in neural network?
In a neural network, "bias" refers to a constant value added to the weighted sum of inputs before passing it through the activation function, essentially allowing the network to shift the decision boundary and learn more complex patterns in the data, while "without bias" means the network would only consider the weighted sum of inputs, limiting its ability to model real-world scenarios where data might not center around the origin;
![bias](https://github.com/user-attachments/assets/5061ddd9-1540-4abb-a7a2-b6f33aaee211)


# Why we need weight in neural network?
Weights set the standards for the neuron's signal strength. This value will determine the influence input data has on the output product.
# What is the weight range of a neural network?
The weights in the network are initialized to small random numbers ranging for example from `-1.0 to 1.0`, or `-0.5 to 0.5`.
